ğŸ¯ Regression-Based Particle Position Detection

In Resistive Silicon Detectors

ğŸ› ï¸ Overview

This project focuses on predicting the position of particles passing through a Resistive Silicon Detector (RSD). By extracting relevant features from signal data, we implemented regression models to accurately determine particle positions. ğŸ§¬âœ¨

ğŸ“‚ Problem Description

The dataset consists of:
	â€¢	ğŸ—‚ï¸ Training Set: 385,500 events with features:
	â€¢	Peak magnitudes (pmax), negative peaks (negpmax), delays (tmax), signal areas (area), and RMS values (rms).
	â€¢	ğŸ“Š Evaluation Set: 128,500 events without target features (x, y).

The goal ğŸ¯: Predict the particleâ€™s x and y positions using the training data.

ğŸš€ Approach

ğŸ§¹ 1. Preprocessing
	â€¢	ğŸ§ª Noise Identification:
	â€¢	Analyzed statistical properties to detect noisy features.
	â€¢	Dropped noisy columns and rare event rows.
	â€¢	ğŸ—ï¸ Feature Selection:
	â€¢	Removed less relevant features (e.g., RMS values) to enhance model performance.

ğŸ¤– 2. Modeling
	â€¢	Models Used:
	â€¢	ğŸŸ¦ Ridge Regression: Baseline linear model.
	â€¢	ğŸŒ² Random Forest Regression: Ensemble tree-based model.
	â€¢	ğŸ XGBoost Regression: Gradient boosting algorithm optimized for speed and accuracy.
	â€¢	Metric:
ğŸ“ Euclidean Distance between actual and predicted particle positions.

ğŸ›ï¸ 3. Hyperparameter Tuning
	â€¢	ğŸ¯ Ridge & Random Forest: Tuned using GridSearchCV.
	â€¢	âš¡ XGBoost: Tuned using Optuna for efficient parameter optimization.

ğŸ“ˆ Results
	â€¢	Ridge Regression: ğŸ¯ Score = 18.157 (Baseline)
	â€¢	Random Forest Regression: ğŸŒŸ Score = 5.170
	â€¢	XGBoost Regression: ğŸ† Best Score = 4.959

ğŸ” Key Insights
	â€¢	ğŸ¥‡ XGBoost provided the best performance due to its adaptability and efficiency.
	â€¢	âœ¨ Preprocessing played a critical role in boosting model accuracy.
	â€¢	ğŸ’¡ Future improvements could involve advanced noise handling and additional optimization techniques.

âš™ï¸ Requirements
	â€¢	Python 3.8+ ğŸ
	â€¢	Required libraries:
numpy, pandas, scikit-learn, xgboost, optuna, matplotlib


ğŸ‘¥ Authors
	â€¢	ğŸ§‘â€ğŸ”¬ Chiara Roberta Casale
âœ‰ï¸ Email: s322574@studenti.polito.it
	â€¢	ğŸ§‘â€ğŸ”¬ Yegane Bagheri
âœ‰ï¸ Email: s327779@studenti.polito.it

ğŸ“š References
	1.	ğŸ“„ Tornago et al., â€œSilicon sensors with resistive read-out: Machine learning techniques for ultimate spatial resolution,â€ Nuclear Instruments and Methods in Physics Research Section A, 2023.
	2.	ğŸ“˜ James et al., An Introduction to Statistical Learning, Springer, 2013.
	3.	ğŸ“Š BÃ©ntÃ©jac et al., â€œA comparative analysis of gradient boosting algorithms,â€ Artificial Intelligence Review, 2021.
	4.	ğŸ” Akiba et al., â€œOptuna: A next-generation hyperparameter optimization framework,â€ Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019.
